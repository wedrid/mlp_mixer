{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b22627",
   "metadata": {},
   "source": [
    "Hyperparameters definition and loading of CIFAR100 from pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e97b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_portion = 0.7\n",
    "\n",
    "#controllare che len(val) Ã¨ len(train)\n",
    "\n",
    "#hyperparameters\n",
    "batch_size = 250\n",
    "\n",
    "#pad_totensor_transform = transforms.Compose([transforms.Pad(2), transforms.ToTensor()]) # does the padding, images 32x32 become 36x36 (symmetric increase) so that are divisible by three and patches are 12x12\n",
    "pad_totensor_transform = transforms.Compose([transforms.ToTensor()]) #no pad, no normalization\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR100(root='./cifar100_data', train=True, transform=pad_totensor_transform, download=True)\n",
    "train_subset, val_subset = torch.utils.data.random_split(dataset, [int(train_portion*len(dataset)), len(dataset) - int(train_portion*len(dataset))], generator=torch.Generator().manual_seed(1))\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./cifar100_data', train=False, transform=pad_totensor_transform)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_subset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_subset, shuffle=False, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9047054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 3, 32, 32]) torch.Size([250])\n",
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test dataloader\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_sample = samples[0]\n",
    "print(img_sample.shape)\n",
    "print(img_sample.shape)\n",
    "plt.imshow(img_sample.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7b597",
   "metadata": {},
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d9992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp_mixer import *\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b8d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predicted, true_labels):\n",
    "    predicted = torch.argmax(predicted, dim=1)\n",
    "    return accuracy_score(predicted, true_labels)\n",
    "\n",
    "def save_model(model):\n",
    "    import time\n",
    "    filename = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = \"./models/mlpmixer_\"+filename+\".pth\"\n",
    "    print(filename)\n",
    "    torch.save(model.state_dict(), filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aae0e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/wedrid/mlp-mixer/92521f82e0ec4f02b3fbc755e17f9f91\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size                   : 250\n",
      "COMET INFO:     epochs                       : 1\n",
      "COMET INFO:     hidden_dim_size (n_channels) : 100\n",
      "COMET INFO:     image_width_and_height       : 32\n",
      "COMET INFO:     learning_rate                : 0.001\n",
      "COMET INFO:     mlp_dc_dimension             : 1024\n",
      "COMET INFO:     mlp_ds_dimension             : 128\n",
      "COMET INFO:     number_of_layers             : 8\n",
      "COMET INFO:     patch_width_and_height       : 8\n",
      "COMET INFO:     steps                        : 140\n",
      "COMET INFO:     train_size                   : 140\n",
      "COMET INFO:     validation_size              : 60\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (41 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/wedrid/mlp-mixer/a2baa9cc0b0347ed8ea2a66e3f4efe3e\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51c7980cc6a498f93183d308f73709f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39dc87aaa3242a09b91dde991256111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 1: 3.8502\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc34e0450bf34c4585a029c831c770c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "experiment = Experiment(\n",
    "    api_key=\"xX6qWBFbiOreu0W3IrO14b9nB\",\n",
    "    project_name=\"mlp-mixer\",\n",
    "    workspace=\"wedrid\",\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "image_width_height = img_sample.shape[1]\n",
    "patch_dims = 4\n",
    "# variable_name = value #paper value\n",
    "n_channels = 100 #512\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "num_layers = 8\n",
    "mlp_dc_dimension = 1024 #2048 # dc Ã¨ la dimensione del channel mixing (l'ultimo mlp)\n",
    "mlp_ds_dimension = 128 #256 # ds Ã¨ la dimensione del token mixing (il primo)\n",
    "\n",
    "model = MLP_mixer(img_h_w=image_width_height, patch_dim=patch_dims, n_channels=n_channels, num_mixers_layers=num_layers,\n",
    "    hidden_dim_mlp_token=mlp_ds_dimension, hidden_dim_mlp_channel=mlp_dc_dimension) #in this case 2 patches 16x16\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "num_epochs = 1\n",
    "steps_total = len(train_loader)\n",
    "\n",
    "hyper_params = {\n",
    "    \"train_size\": len(train_loader),\n",
    "    \"validation_size\": len(val_loader),\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"steps\": steps_total,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"image_width_and_height\": image_width_height,\n",
    "    \"patch_width_and_height\": patch_dims,\n",
    "    \"hidden_dim_size (n_channels)\": n_channels,\n",
    "    \"number_of_layers\": num_layers,\n",
    "    \"mlp_dc_dimension\": mlp_dc_dimension,\n",
    "    \"mlp_ds_dimension\": mlp_ds_dimension\n",
    "}\n",
    "\n",
    "experiment.log_parameters(hyper_params)\n",
    "\n",
    "# training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    train_accuracy = 0\n",
    "    for i, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "        # [100, 3, 36, 36] is what is returned by iterator\n",
    "        images = images.to(device)\n",
    "        true_labels = labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        predicted = model(images)\n",
    "        loss = loss_func(predicted, true_labels)\n",
    "        train_accuracy += get_accuracy(predicted, true_labels)\n",
    "\n",
    "        # backwards pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if False and (i+1) % 100:\n",
    "            print(f'epoch: {epoch+1} of {num_epochs}, step {i+1} of {steps_total}, loss = {loss.item():.4f}')\n",
    "    print(f\"Loss of epoch {epoch+1}: {loss.item():.4f}\")\n",
    "    train_accuracy /= len(train_loader)\n",
    "    experiment.log_metric(\"train epoch loss\", loss.item(), step=epoch)\n",
    "    experiment.log_metric(\"mean train epoch accuracy\", train_accuracy, step=epoch)\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_accuracy = 0\n",
    "        for i, (images, labels) in enumerate(tqdm(val_loader)):\n",
    "            # [100, 3, 36, 36] is what is returned by iterator\n",
    "            images = images.to(device)\n",
    "            true_labels = labels.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            predicted = model(images)\n",
    "            loss = loss_func(predicted, true_labels)\n",
    "            val_accuracy += get_accuracy(predicted, true_labels)\n",
    "        val_accuracy /= len(val_loader)\n",
    "        experiment.log_metric(\"val epoch loss\", loss.item(), step=epoch)\n",
    "        experiment.log_metric(\"mean val epoch accuracy\", val_accuracy, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ddcc6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/mlpmixer_20220308-182921.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84652e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 100])\n",
      "Accuracy 0.116\n",
      "torch.Size([250])\n",
      "tensor([49, 53, 95, 86,  0, 73, 36,  0, 27, 36, 66, 36, 95, 20, 24, 36,  0, 69,\n",
      "        68,  7, 73, 47,  5, 36, 18, 34, 73, 34,  0, 54, 36, 41, 17, 34, 17, 42,\n",
      "        53, 60, 36,  0, 36, 78,  0, 36, 52, 57, 17,  0, 51, 73,  5, 47,  0, 53,\n",
      "        52, 78, 53, 36, 21, 78,  7, 36, 86, 60, 27, 53, 64, 36, 52, 52, 69, 27,\n",
      "        37, 69, 69, 69,  0, 36, 36, 42, 43,  5, 69, 41, 37, 89, 53, 37, 36, 69,\n",
      "        42, 47, 60, 22, 24, 52, 57, 94, 53, 34, 95, 34, 34, 16, 24,  3,  3, 95,\n",
      "        78, 77, 42, 18, 61, 17, 34, 69, 73, 33, 86, 36, 54, 34, 36, 34, 36, 21,\n",
      "         5, 69, 60, 53, 60, 42, 60, 17, 31, 53, 36, 41, 53, 69, 68, 36, 94, 31,\n",
      "         3,  7, 62, 68, 52, 36, 41, 47, 34, 34, 95, 82, 60,  3, 54, 53, 17, 36,\n",
      "        92, 37,  5, 73, 60, 41, 89, 92, 36, 18, 21, 36, 86, 43, 49, 42, 62, 73,\n",
      "        18, 97, 95,  0, 43,  0, 56, 97, 24, 69, 43, 54, 18, 83, 36, 48, 43, 60,\n",
      "        37, 17, 54, 52, 34, 54, 82,  3, 27, 34, 34, 43, 24,  3, 68, 36, 41, 85,\n",
      "        53, 47, 43, 52, 68, 69, 52, 21, 24, 36, 90, 86, 78, 60, 60, 37, 21, 36,\n",
      "        57, 31,  0, 41,  3, 34, 15, 42, 43, 18, 27, 36, 36, 33, 36, 68])\n"
     ]
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "images, labels = examples.next()\n",
    "\n",
    "# metrics trial\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(images)\n",
    "loss = loss_func(outputs, labels)\n",
    "\n",
    "#da mettere nel ciclo\n",
    "print(outputs.shape)\n",
    "\n",
    "#####\n",
    "predicted = torch.argmax(outputs, dim=1)\n",
    "accuracy = accuracy_score(predicted, labels)\n",
    "print(f'Accuracy {accuracy}')\n",
    "#####\n",
    "print(predicted.shape)\n",
    "print(predicted)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c12a9f36891af4c5b0dfce9e1e49ba1e8afd45b3a8e9c06689a1b7faea4e57c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b22627",
   "metadata": {},
   "source": [
    "Hyperparameters definition and loading of CIFAR100 from pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e97b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_portion = 0.7\n",
    "\n",
    "#controllare che len(val) è len(train)\n",
    "\n",
    "#hyperparameters\n",
    "batch_size = 250\n",
    "\n",
    "#pad_totensor_transform = transforms.Compose([transforms.Pad(2), transforms.ToTensor()]) # does the padding, images 32x32 become 36x36 (symmetric increase) so that are divisible by three and patches are 12x12\n",
    "pad_totensor_transform = transforms.Compose([transforms.ToTensor()]) #no pad, no normalization\n",
    "\n",
    "root = './cifar100_data' #if not in lab\n",
    "root = '../datasets/cifar100'\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR100(root=root, train=True, transform=pad_totensor_transform, download=True)\n",
    "train_subset, val_subset = torch.utils.data.random_split(dataset, [int(train_portion*len(dataset)), len(dataset) - int(train_portion*len(dataset))], generator=torch.Generator().manual_seed(1))\n",
    "test_dataset = torchvision.datasets.CIFAR100(root=root, train=False, transform=pad_totensor_transform)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_subset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_subset, shuffle=False, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9047054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 3, 32, 32]) torch.Size([250])\n",
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test dataloader\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_sample = samples[0]\n",
    "print(img_sample.shape)\n",
    "print(img_sample.shape)\n",
    "plt.imshow(img_sample.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7b597",
   "metadata": {},
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21d9992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp_mixer import *\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18b8d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predicted, true_labels):\n",
    "    predicted = torch.argmax(predicted, dim=1)\n",
    "    return accuracy_score(predicted, true_labels)\n",
    "\n",
    "def generate_folder():\n",
    "    import time\n",
    "    import os\n",
    "    import os.path\n",
    "    datetime = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    folder = f\"./models/{datetime}/\"\n",
    "    os.chdir(\".\")\n",
    "    print(\"current dir is: %s\" % (os.getcwd()))\n",
    "\n",
    "    if os.path.isdir(folder):\n",
    "        print(\"Exists\")\n",
    "    else:\n",
    "        os.mkdir(folder)\n",
    "    return folder\n",
    "\n",
    "#def save_model(model, path):\n",
    "#    filename = path + f\"{filename}.pth\"\n",
    "#    print(filename)\n",
    "#    torch.save(model.state_dict(), filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aae0e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/wedrid/mlp-mixer/065184557eb84f91beabc42224a8fe78\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     mean train epoch accuracy [10] : (0.022085714285714302, 0.13248571428571423)\n",
      "COMET INFO:     mean val epoch accuracy [10]   : (0.03373333333333336, 0.13133333333333333)\n",
      "COMET INFO:     train epoch loss [10]          : (3.595264196395874, 4.48098611831665)\n",
      "COMET INFO:     val epoch loss [10]            : (3.748150587081909, 4.461355209350586)\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size                   : 250\n",
      "COMET INFO:     epochs                       : 10\n",
      "COMET INFO:     hidden_dim_size (n_channels) : 10\n",
      "COMET INFO:     image_width_and_height       : 32\n",
      "COMET INFO:     learning_rate                : 0.001\n",
      "COMET INFO:     mlp_dc_dimension             : 8\n",
      "COMET INFO:     mlp_ds_dimension             : 8\n",
      "COMET INFO:     number_of_layers             : 3\n",
      "COMET INFO:     patch_width_and_height       : 4\n",
      "COMET INFO:     steps                        : 140\n",
      "COMET INFO:     train_size                   : 140\n",
      "COMET INFO:     validation_size              : 60\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (55 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/wedrid/mlp-mixer/c64fda0880cd4a2b97e76e4541019fe6\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dir is: /Users/edrid/Desktop/Machine learning/Project/mlp_mixer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff7fb80a2d540faa0a29b6f3be85e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24e2642b8154cb3bca2c708803e059d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 1: 4.5185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5500f6c51745fb870042e2aa377a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e060d9f87b8e4a528959a71f669b2da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 2: 4.2359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ddd007aec94eafbda81781f77ba96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd08dd1eb1bb4d7ba8f9ed861b1f4c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 3: 4.0947\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c12d7e2245492a9439c6b4615dd069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b3958a57e4456983f72381ffd8c930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 4: 3.8404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8d014c94cf48ba90876680037cf299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6596b043b18f4d3699d3017f29c5f4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 5: 3.8337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503811303ee14a3180c594d49521eacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aee74a1627a43869041974df1fcf2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 6: 3.7143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56820683f896452bbc4556bf1b356e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633c9ad558704174a4d50fa6529af71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 7: 3.6156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b29024365c4d90a4a082c6dea69402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4cc24fcc724124ace83a0f6c6f674b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 8: 3.5582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c36648551114fc29edcd21a6112d88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29109306713f482b804deee290448a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 9: 3.5231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97eb1b4c228d48ee9aaf213602412813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db4964555f04cdc8a31e6031dc0ae48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 10: 3.5575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de23cfebe68407dac7c149940a7de71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "experiment = Experiment(\n",
    "    api_key=\"xX6qWBFbiOreu0W3IrO14b9nB\",\n",
    "    project_name=\"mlp-mixer\",\n",
    "    workspace=\"wedrid\",\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "image_width_height = img_sample.shape[1]\n",
    "patch_dims = 4\n",
    "# variable_name = value #paper value\n",
    "n_channels = 10 #100 #512\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "num_layers = 3 #8\n",
    "mlp_dc_dimension = 32 #1024 #2048 # dc è la dimensione del channel mixing (l'ultimo mlp)\n",
    "mlp_ds_dimension = 32 #128 #256 # ds è la dimensione del token mixing (il primo)\n",
    "\n",
    "model = MLP_mixer(img_h_w=image_width_height, patch_dim=patch_dims, n_channels=n_channels, num_mixers_layers=num_layers,\n",
    "    hidden_dim_mlp_token=mlp_ds_dimension, hidden_dim_mlp_channel=mlp_dc_dimension) #in this case 2 patches 16x16\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "steps_total = len(train_loader)\n",
    "\n",
    "#ATTENZIONE: CAMBIARE IPERPARAMETRI ***PRIMAAAA*** DEL DICT SUCCESSIVO\n",
    "\n",
    "hyper_params = {\n",
    "    \"train_size\": len(train_loader),\n",
    "    \"validation_size\": len(val_loader),\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"steps\": steps_total,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"image_width_and_height\": image_width_height,\n",
    "    \"patch_width_and_height\": patch_dims,\n",
    "    \"hidden_dim_size (n_channels)\": n_channels,\n",
    "    \"number_of_layers\": num_layers,\n",
    "    \"mlp_dc_dimension\": mlp_dc_dimension,\n",
    "    \"mlp_ds_dimension\": mlp_ds_dimension\n",
    "}\n",
    "\n",
    "experiment.log_parameters(hyper_params)\n",
    "model_path = generate_folder()\n",
    "with open(model_path+\"/params.json\", \"w\") as file:\n",
    "    json.dump(hyper_params, file, indent=4)\n",
    "\n",
    "# training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    train_accuracy = 0\n",
    "    for i, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "        # [100, 3, 36, 36] is what is returned by iterator\n",
    "        images = images.to(device)\n",
    "        true_labels = labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        predicted = model(images)\n",
    "        loss = loss_func(predicted, true_labels)\n",
    "        train_accuracy += get_accuracy(predicted, true_labels)\n",
    "\n",
    "        # backwards pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if False and (i+1) % 100:\n",
    "            print(f'epoch: {epoch+1} of {num_epochs}, step {i+1} of {steps_total}, loss = {loss.item():.4f}')\n",
    "    print(f\"Loss of epoch {epoch+1}: {loss.item():.4f}\")\n",
    "    train_accuracy /= len(train_loader)\n",
    "    experiment.log_metric(\"train epoch loss\", loss.item(), step=epoch)\n",
    "    experiment.log_metric(\"mean train epoch accuracy\", train_accuracy, step=epoch)\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_accuracy = 0\n",
    "        for i, (images, labels) in enumerate(tqdm(val_loader)):\n",
    "            # [100, 3, 36, 36] is what is returned by iterator\n",
    "            images = images.to(device)\n",
    "            true_labels = labels.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            predicted = model(images)\n",
    "            loss = loss_func(predicted, true_labels)\n",
    "            val_accuracy += get_accuracy(predicted, true_labels)\n",
    "        val_accuracy /= len(val_loader)\n",
    "        experiment.log_metric(\"val epoch loss\", loss.item(), step=epoch)\n",
    "        experiment.log_metric(\"mean val epoch accuracy\", val_accuracy, step=epoch)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        torch.save(model.state_dict(), model_path + f\"checkpoint_epch_{epoch}.pth\")\n",
    "torch.save(model.state_dict(), model_path + f\"final.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84652e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 100])\n",
      "Accuracy 0.22\n",
      "torch.Size([250])\n",
      "tensor([24, 41, 14,  1, 89, 95, 16, 20, 30, 30, 57, 27,  8, 68, 71, 40, 82, 70,\n",
      "        47, 93, 24, 62, 91, 17, 33, 90, 71, 20, 87,  7, 23, 70,  5, 68, 41, 91,\n",
      "        17, 82, 33, 97, 73, 52,  4, 82, 52, 64, 42, 71, 38, 36, 92, 30, 90,  0,\n",
      "        60, 76, 85, 67, 76, 95, 35, 67, 33, 69, 36, 37, 63, 20, 92, 86, 64, 28,\n",
      "        68,  0, 48, 52, 21, 28, 37, 56, 53, 20, 52, 24, 56, 60, 51, 62, 48, 53,\n",
      "        96, 91, 39, 87, 61,  8, 23, 68, 44, 61,  6, 59, 93,  9, 42, 22, 63, 68,\n",
      "        96, 24, 88, 99, 75, 76, 60, 85, 43, 12, 33, 97, 52, 53, 76, 48, 63, 17,\n",
      "        22, 15, 91, 39, 62, 68, 51, 83, 33, 82, 74,  2, 12, 74, 85, 83, 64, 28,\n",
      "        53, 12, 88, 61, 83, 61, 39,  8, 38, 62, 85, 61, 49, 63, 49, 18, 85,  5,\n",
      "        24, 66, 98, 53, 56, 87, 42, 31, 91, 61, 64, 69, 45, 37, 94, 61, 47, 94,\n",
      "         6, 56, 12, 60,  5, 76, 14, 41, 88, 17, 67, 21,  9, 29, 16, 85, 79, 41,\n",
      "        79,  2, 65, 23, 12, 36, 53, 93, 96, 86, 73, 61, 27, 63, 12, 48, 60, 94,\n",
      "        16, 14, 67, 23, 37, 59,  7, 73, 76, 86, 24, 23, 67, 28, 98, 70, 78,  1,\n",
      "        91, 88, 23, 51, 31,  8, 33, 68, 21, 39, 20, 82, 18, 60, 55, 31])\n"
     ]
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "images, labels = examples.next()\n",
    "\n",
    "# metrics trial\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(images)\n",
    "loss = loss_func(outputs, labels)\n",
    "\n",
    "#da mettere nel ciclo\n",
    "print(outputs.shape)\n",
    "\n",
    "#####\n",
    "predicted = torch.argmax(outputs, dim=1)\n",
    "accuracy = accuracy_score(predicted, labels)\n",
    "print(f'Accuracy {accuracy}')\n",
    "#####\n",
    "print(predicted.shape)\n",
    "print(predicted)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c12a9f36891af4c5b0dfce9e1e49ba1e8afd45b3a8e9c06689a1b7faea4e57c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

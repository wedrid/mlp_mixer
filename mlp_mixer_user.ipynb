{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fadca5b",
   "metadata": {},
   "source": [
    "Hyperparameters definition and loading of CIFAR100 from pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8049fb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "BATCH SIZE: 250\n",
      "Tran subset len: 35000\n",
      "Tran loader len: 140\n",
      "Test: 140.0\n",
      "Val subset len: 15000\n",
      "Val subset len: 60\n",
      "Test: 60.0\n",
      "Test subset len: 10000\n",
      "Test subset len: 40\n",
      "Test: 40.0\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_portion = 0.7\n",
    "\n",
    "#controllare che len(val) è len(train)\n",
    "\n",
    "#hyperparameters\n",
    "batch_size = 250\n",
    "\n",
    "#pad_totensor_transform = transforms.Compose([transforms.Pad(2), transforms.ToTensor()]) # does the padding, images 32x32 become 36x36 (symmetric increase) so that are divisible by three and patches are 12x12\n",
    "#pad_totensor_transform = transforms.Compose([transforms.ToTensor()]) #no pad, no normalization\n",
    "randAugm_numops = 2\n",
    "randAugm_magn = 15\n",
    "my_transforms = transforms.Compose([ transforms.RandAugment(num_ops = randAugm_numops,magnitude = randAugm_magn ),\n",
    "                            transforms.ToTensor(), #nota importante, ToTensor dev'essere sempre come ultima trasformazione\n",
    "                            ])\n",
    "\n",
    "root = './cifar100_data' #if not in lab\n",
    "#root = '../datasets/cifar100'\n",
    "\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR100(root=root, train=True, transform = my_transforms, download=True)\n",
    "train_subset, val_subset = torch.utils.data.random_split(dataset, [int(train_portion*len(dataset)), len(dataset) - int(train_portion*len(dataset))], generator=torch.Generator().manual_seed(1))\n",
    "test_dataset = torchvision.datasets.CIFAR100(root=root, train=False, transform = my_transforms)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_subset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_subset, shuffle=False, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"BATCH SIZE: {batch_size}\")\n",
    "print(f\"Tran subset len: {len(train_subset)}\")\n",
    "print(f\"Tran loader len: {len(train_loader)}\")\n",
    "print(f\"Test: {len(train_subset)/batch_size}\")\n",
    "\n",
    "print(f\"Val subset len: {len(val_subset)}\")\n",
    "print(f\"Val subset len: {len(val_loader)}\")\n",
    "print(f\"Test: {len(val_subset)/batch_size}\")\n",
    "\n",
    "\n",
    "print(f\"Test subset len: {len(test_dataset)}\")\n",
    "print(f\"Test subset len: {len(test_loader)}\")\n",
    "print(f\"Test: {len(test_dataset)/batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9284dbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 3, 32, 32]) torch.Size([250])\n",
      "torch.Size([3, 32, 32])\n",
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test dataloader\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_sample = samples[0]\n",
    "print(img_sample.shape)\n",
    "print(img_sample.shape)\n",
    "plt.imshow(img_sample.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f664da",
   "metadata": {},
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46e2c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install einops\n",
    "from mlp_mixer import *\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d64618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predicted, true_labels):\n",
    "    predicted = torch.argmax(predicted.cpu(), dim=1)\n",
    "    return accuracy_score(predicted, true_labels.cpu()) #forse questi passaggi a cpu non sono molto efficienti..\n",
    "\n",
    "def generate_folder():\n",
    "    import time\n",
    "    import os\n",
    "    import os.path\n",
    "    datetime = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    folder = f\"./models/{datetime}/\"\n",
    "    os.chdir(\".\")\n",
    "    print(\"current dir is: %s\" % (os.getcwd()))\n",
    "\n",
    "    if os.path.isdir(folder):\n",
    "        print(\"Exists\")\n",
    "    else:\n",
    "        os.mkdir(folder)\n",
    "    return folder\n",
    "\n",
    "#def save_model(model, path):\n",
    "#    filename = path + f\"{filename}.pth\"\n",
    "#    print(filename)\n",
    "#    torch.save(model.state_dict(), filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3a4387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py \n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    \n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a231bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/wedrid/mlp-mixer/06407738ff2b4128a1d51f998454715b\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size                   : 250\n",
      "COMET INFO:     comment                      : added weight decay\n",
      "COMET INFO:     dataset                      : ./cifar100_data\n",
      "COMET INFO:     epochs                       : 500\n",
      "COMET INFO:     hidden_dim_size (n_channels) : 32\n",
      "COMET INFO:     image_width_and_height       : 32\n",
      "COMET INFO:     learning_rate                : 0.001\n",
      "COMET INFO:     mlp_dc_dimension             : 128\n",
      "COMET INFO:     mlp_ds_dimension             : 32\n",
      "COMET INFO:     number_of_layers             : 5\n",
      "COMET INFO:     patch_width_and_height       : 4\n",
      "COMET INFO:     steps                        : 140\n",
      "COMET INFO:     train_size                   : 140\n",
      "COMET INFO:     validation_size              : 60\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (87 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/wedrid/mlp-mixer/8fc166034c5a4c60badb301aaaa14841\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dir is: /Users/edrid/Desktop/Machine learning/Project/mlp_mixer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9d2ea3ae3447b7a2e682392ba531e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 1: 4.5837\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80e0a4acf414098b8a389d0e87141f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 2: 4.5493\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6387317d67244ce6bc040a142f55ee72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of epoch 3: 4.5665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486f46c24cf5458d9ef0beb9ac4dad23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-360fca2e491d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#for i, (images, labels) in enumerate(tqdm(train_loader)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m# [100, 3, 36, 36] is what is returned by iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/autoaugment.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mmagnitudes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mmagnitude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagnitudes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagnitude\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmagnitudes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msigned\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m                 \u001b[0mmagnitude\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_apply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "experiment = Experiment(\n",
    "    api_key=\"xX6qWBFbiOreu0W3IrO14b9nB\",\n",
    "    project_name=\"mlp-mixer\",\n",
    "    workspace=\"wedrid\",\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "image_width_height = img_sample.shape[1]\n",
    "patch_dims = 4\n",
    "# variable_name = value #paper value\n",
    "n_channels = 32 #128 #256 #100 #512 #embed dim\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.1\n",
    "num_layers = 7 #8\n",
    "mlp_dc_dimension = 512 #512 #1024 #2048 # dc è la dimensione del channel mixing (l'ultimo mlp)\n",
    "mlp_ds_dimension = 64 #64 #128 #256 # ds è la dimensione del token mixing (il primo)\n",
    "\n",
    "mixup = True\n",
    "mixup_alpha = 0.5\n",
    "\n",
    "model = MLP_mixer(img_h_w=image_width_height, patch_dim=patch_dims, n_channels=n_channels, num_mixers_layers=num_layers,\n",
    "    hidden_dim_mlp_token=mlp_ds_dimension, hidden_dim_mlp_channel=mlp_dc_dimension) #in this case 2 patches 16x16\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay) \n",
    "\n",
    "num_epochs = 500\n",
    "steps_total = len(train_loader)\n",
    "\n",
    "#ATTENZIONE: CAMBIARE IPERPARAMETRI ***PRIMAAAA*** DEL DICT SUCCESSIVO\n",
    "\n",
    "hyper_params = {\n",
    "    \"dataset\": root,\n",
    "    \"rand_augm_numops\": randAugm_numops,\n",
    "    \"rand_augm_magnitude\": randAugm_magn,\n",
    "    \"comment\": 'added weight decay',\n",
    "    \"train_size\": len(train_loader),\n",
    "    \"validation_size\": len(val_loader),\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"steps\": steps_total,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"mixup_alpha\": mixup_alpha, \n",
    "    \"image_width_and_height\": image_width_height,\n",
    "    \"patch_width_and_height\": patch_dims,\n",
    "    \"hidden_dim_size (n_channels)\": n_channels,\n",
    "    \"number_of_layers\": num_layers,\n",
    "    \"mlp_dc_dimension\": mlp_dc_dimension,\n",
    "    \"mlp_ds_dimension\": mlp_ds_dimension\n",
    "}\n",
    "\n",
    "experiment.log_parameters(hyper_params)\n",
    "model_path = generate_folder()\n",
    "with open(model_path+\"/params.json\", \"w\") as file:\n",
    "    json.dump(hyper_params, file, indent=4)\n",
    "\n",
    "model.to(device)\n",
    "# training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    train_accuracy = 0\n",
    "    #for i, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # [100, 3, 36, 36] is what is returned by iterator\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if not mixup: #without mixup regularization\n",
    "            # forward pass\n",
    "            predicted = model(images)\n",
    "            loss = loss_func(predicted, labels)\n",
    "            \n",
    "        else:\n",
    "            images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=mixup_alpha)\n",
    "            #images, labels_a, labels_b = map(Variable, ) no because Variable is deprecated\n",
    "            predicted = model(images)\n",
    "            loss = mixup_criterion(loss_func, predicted, labels_a, labels_b, lam)\n",
    "            \n",
    "        train_accuracy += get_accuracy(predicted, labels)\n",
    "        # backwards pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if False and (i+1) % 100:\n",
    "            print(f'epoch: {epoch+1} of {num_epochs}, step {i+1} of {steps_total}, loss = {loss.item():.4f}')\n",
    "    print(f\"Loss of epoch {epoch+1}: {loss.item():.4f}\")\n",
    "    train_accuracy /= len(train_loader)\n",
    "    #print(f\"TRAIN LOADER LENGTH: {len(train_loader)}\")\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    experiment.log_metric(\"train epoch loss\", loss.item(), step = epoch)\n",
    "    experiment.log_metric(\"mean train epoch accuracy\", train_accuracy, step = epoch)\n",
    "    experiment.log_metric(\"epoch time\", elapsed, step = epoch)\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_accuracy = 0\n",
    "        temp = 0\n",
    "        for i, (images, labels) in enumerate(tqdm(val_loader)): #numero esempi/batchsize TODO check\n",
    "            # [100, 3, 36, 36] is what is returned by iterator\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            predicted = model(images)\n",
    "            loss = loss_func(predicted, labels)\n",
    "            val_accuracy += get_accuracy(predicted, labels)\n",
    "        #print(f\"Lenght val loader: {len(val_loader)}, counter: {temp}\")\n",
    "        val_accuracy /= len(val_loader) \n",
    "        experiment.log_metric(\"val epoch loss\", loss.item(), step=epoch) #TODO average loss?\n",
    "        experiment.log_metric(\"mean val epoch accuracy\", val_accuracy, step=epoch)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), model_path + f\"checkpoint_epch_{epoch}.pth\")\n",
    "torch.save(model.state_dict(), model_path + f\"final.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    examples = iter(train_loader)\n",
    "    images, labels = examples.next()\n",
    "\n",
    "    # metrics trial\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(images)\n",
    "    loss = loss_func(outputs, labels)\n",
    "\n",
    "    #da mettere nel ciclo\n",
    "    print(outputs.shape)\n",
    "\n",
    "    #####\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = accuracy_score(predicted, labels)\n",
    "    print(f'Accuracy {accuracy}')\n",
    "    #####\n",
    "    print(predicted.shape)\n",
    "    print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a9380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c12a9f36891af4c5b0dfce9e1e49ba1e8afd45b3a8e9c06689a1b7faea4e57c"
  },
  "kernelspec": {
   "display_name": "ViT_MLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from mlp_mixer import * \n",
    "import torch\n",
    "from get_dataloaders import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "path = \"./models/20220401-122435/\"\n",
    "with open(path+'out_hyperparams.json') as json_file:\n",
    "    params = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5a5180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_mixer(\n",
       "  (patch_embedder): Conv2d(3, 128, kernel_size=(8, 8), stride=(8, 8))\n",
       "  (mixerlayers): ModuleList(\n",
       "    (0): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_fc_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc_head): Linear(in_features=128, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP_mixer(img_h_w=params['image_width_and_height'], patch_dim=params['patch_width_and_height'], n_channels=params['hidden_dim_size (n_channels)'], num_mixers_layers=params['number_of_layers'],\n",
    "    hidden_dim_mlp_token=params['mlp_ds_dimension'], hidden_dim_mlp_channel=params['mlp_dc_dimension'], n_classes=1000)\n",
    "\n",
    "model.load_state_dict(torch.load(path+\"checkpoint_epch_300.pth\", map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ded7fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "BATCH SIZE: 512\n",
      "Tran subset len: 50000\n",
      "Tran loader len: 98\n",
      "Test: 97.65625\n",
      "Val/test subset len: 10000\n",
      "Val/test subset len: 20\n",
      "Val/Test: 19.53125\n",
      "Test subset len: 10000\n",
      "Test subset len: 20\n",
      "Test: 19.53125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP_mixer(\n",
       "  (patch_embedder): Conv2d(3, 128, kernel_size=(8, 8), stride=(8, 8))\n",
       "  (mixerlayers): ModuleList(\n",
       "    (0): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): MixerLayer(\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (MLP1): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (MLP2): MLP(\n",
       "        (linear_gelu_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_fc_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc_head): Linear(in_features=128, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['batch_size'] = 512\n",
    "train_loader, val_loader, num_classes = getUpsampledCIFAR100Loaders(params)\n",
    "num_in_features = model.fc_head.in_features\n",
    "\n",
    "#cifar has 100 classes\n",
    "model.fc_head = nn.Linear(num_in_features, num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41cb418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyperparameters for the fine tuning\n",
    "new_params = {'learning_rate': 0.001, 'weight_decay': 1e-5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf307a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/wedrid/mlp-mixer-finetune/dd885817d639418cadc261120939bcab\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eab777a23ab41bea19dc18090107996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc893db86ab474889e918a0ad6706ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from comet_ml import Experiment\n",
    "\n",
    "experiment = Experiment(\n",
    "    api_key=\"xX6qWBFbiOreu0W3IrO14b9nB\",\n",
    "    project_name=\"mlp-mixer-finetune\",\n",
    "    workspace=\"wedrid\",\n",
    ")\n",
    "\n",
    "log = True\n",
    "num_epochs = 500\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = new_params['learning_rate'], weight_decay=new_params['weight_decay'])\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "# training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    train_accuracy = 0\n",
    "    for i, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "        # [100, 3, 36, 36] is what is returned by iterator\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        predicted = model(images)\n",
    "        loss = loss_func(predicted, labels)\n",
    "        train_accuracy += ((predicted.argmax(dim=-1) == labels).float().mean()).item()\n",
    "\n",
    "        # backwards pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if False and (i+1) % 100:\n",
    "            print(f'epoch: {epoch+1} of {num_epochs}, step {i+1} of {steps_total}, loss = {loss.item():.4f}')\n",
    "    print(f\"Loss of epoch {epoch+1}: {loss.item():.4f}\")\n",
    "    train_accuracy /= len(train_loader)\n",
    "    #print(f\"TRAIN LOADER LENGTH: {len(train_loader)}\")\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    if log: \n",
    "        experiment.log_metric(\"train epoch loss\", loss.item(), step=epoch)\n",
    "        experiment.log_metric(\"mean train epoch accuracy\", train_accuracy, step=epoch)\n",
    "        experiment.log_metric(\"epoch time\", elapsed, step = epoch)\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_accuracy = 0\n",
    "        temp = 0\n",
    "        for i, (images, labels) in enumerate(tqdm(val_loader)): #numero esempi/batchsize TODO check\n",
    "            # [100, 3, 36, 36] is what is returned by iterator\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            predicted = model(images)\n",
    "            loss = loss_func(predicted, labels)\n",
    "            val_accuracy += ((predicted.argmax(dim=-1) == labels).float().mean()).item()\n",
    "        #print(f\"Lenght val loader: {len(val_loader)}, counter: {temp}\")\n",
    "        val_accuracy /= len(val_loader) \n",
    "        if log: \n",
    "            experiment.log_metric(\"val epoch loss\", loss.item(), step=epoch)\n",
    "            experiment.log_metric(\"mean val epoch accuracy\", val_accuracy, step=epoch)\n",
    "    \n",
    "    if epoch % 10 == 0 and False:\n",
    "        torch.save(model.state_dict(), model_path + f\"checkpoint_epch_{epoch}.pth\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c12a9f36891af4c5b0dfce9e1e49ba1e8afd45b3a8e9c06689a1b7faea4e57c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
